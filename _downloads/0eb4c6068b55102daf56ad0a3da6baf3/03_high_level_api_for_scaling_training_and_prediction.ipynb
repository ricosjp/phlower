{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n# High Level API for scaling, training and prediction\n\nIn this section, we will use high level API for performing machine learning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At First, we will prepare dummy data.\nThese dummy data corresponds to feature values extracted from simultion data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\nimport random\nimport shutil\n\nimport numpy as np\nimport scipy.sparse as sp\n\n\ndef prepare_sample_interim_files():\n    np.random.seed(0)\n    random.seed(0)\n\n    output_directory = pathlib.Path(\"out\")\n    if output_directory.exists():\n        shutil.rmtree(output_directory)\n\n    base_interim_dir = output_directory / \"interim\"\n    base_interim_dir.mkdir(parents=True)\n\n    n_cases = 5\n    dtype = np.float32\n    for i in range(n_cases):\n        n_nodes = 100 * (i + 1)\n        interim_dir = base_interim_dir / f\"case_{i}\"\n        interim_dir.mkdir()\n\n        nodal_initial_u = np.random.rand(n_nodes, 3, 1)\n        np.save(\n            interim_dir / \"nodal_initial_u.npy\",\n            nodal_initial_u.astype(dtype),\n        )\n\n        # nodal_last_u = np.random.rand(n_nodes, 3, 1)\n        np.save(interim_dir / \"nodal_last_u.npy\", nodal_initial_u.astype(dtype))\n\n        sparse_array_names = [\n            \"nodal_nadj\",\n            \"nodal_x_grad_hop1\",\n            \"nodal_y_grad_hop1\",\n            \"nodal_z_grad_hop1\",\n        ]\n        rng = np.random.default_rng()\n        for name in sparse_array_names:\n            arr = sp.random(n_nodes, n_nodes, density=0.1, random_state=rng)\n            sp.save_npz(interim_dir / name, arr.tocoo().astype(dtype))\n\n        (interim_dir / \"converted\").touch()\n\n\nprepare_sample_interim_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting file for scaling and training can be downloaded from\n[data.yml](https://github.com/ricosjp/phlower/tutorials/basic_usages/sample_data/e2e/setting.yml)\nwe perform scaling process for data above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from phlower.services.preprocessing import PhlowerScalingService\nfrom phlower.settings import PhlowerSetting\n\nsetting = PhlowerSetting.read_yaml(\"sample_data/e2e/setting.yml\")\n\nscaler = PhlowerScalingService.from_setting(setting)\nscaler.fit_transform_all(\n    interim_data_directories=[\n        pathlib.Path(\"out/interim/case_0\"),\n        pathlib.Path(\"out/interim/case_1\"),\n        pathlib.Path(\"out/interim/case_2\"),\n        pathlib.Path(\"out/interim/case_3\"),\n        pathlib.Path(\"out/interim/case_4\"),\n    ],\n    output_base_directory=pathlib.Path(\"out/preprocessed\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we perform training by using preprocessed data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from phlower.services.trainer import PhlowerTrainer\n\ntrainer = PhlowerTrainer.from_setting(setting)\n\nloss = trainer.train(\n    train_directories=[\n        pathlib.Path(\"out/preprocessed/case_0\"),\n        pathlib.Path(\"out/preprocessed/case_1\"),\n        pathlib.Path(\"out/preprocessed/case_2\"),\n    ],\n    validation_directories=[\n        pathlib.Path(\"out/preprocessed/case_3\"),\n        pathlib.Path(\"out/preprocessed/case_4\"),\n    ],\n    output_directory=pathlib.Path(\"out/model\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``train`` function returns float which corresponds to last training loss.\nLet's call print it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we perform predicion by using pretrained model.\nSetting file for prediction can be downloaded from\n[data.yml](https://github.com/ricosjp/phlower/tutorials/basic_usages/sample_data/e2e/predict.yml)\n\nIt is found that physical dimension is also considered properly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from phlower.services.predictor import PhlowerPredictor\n\nsetting = PhlowerSetting.read_yaml(\"sample_data/e2e/predict.yml\")\n\npredictor = PhlowerPredictor(\n    model_directory=pathlib.Path(\"out/model\"),\n    predict_setting=setting.prediction,\n)\n\npreprocessed_directories = [pathlib.Path(\"out/preprocessed/case_3\")]\n\nfor result in predictor.predict(\n    preprocessed_directories, perform_inverse_scaling=False\n):\n    for k in result.prediction_data.keys():\n        print(f\"{k}: {result.prediction_data[k].dimension}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}